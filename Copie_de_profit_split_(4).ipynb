{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erwannlenoach/notepad/blob/master/Copie_de_profit_split_(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMWO6bA4x1YT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Generate Synthetic Data\n",
        "def generate_synthetic_data(num_samples=10000, seed=42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Define industries and functions\n",
        "    industries = ['manufacturing', 'services', 'technology', 'retail']\n",
        "    functions = ['manufacturing', 'R&D', 'marketing', 'sales']\n",
        "\n",
        "    # Assign industries and functions to headquarters and subsidiaries\n",
        "    hq_industry = np.random.choice(industries, num_samples)\n",
        "    subs_industry = np.random.choice(industries, num_samples)\n",
        "    hq_function = np.random.choice(functions, num_samples)\n",
        "    subs_function = np.random.choice(functions, num_samples)\n",
        "\n",
        "    # Define industry-specific multipliers for revenue, cost, assets, and liabilities\n",
        "    industry_multipliers = {\n",
        "        'manufacturing': {'revenue': 1.0, 'cost': 0.7, 'assets': 1.2, 'liabilities': 0.8},\n",
        "        'services': {'revenue': 0.9, 'cost': 0.6, 'assets': 1.0, 'liabilities': 0.7},\n",
        "        'technology': {'revenue': 1.2, 'cost': 0.8, 'assets': 1.5, 'liabilities': 0.9},\n",
        "        'retail': {'revenue': 0.8, 'cost': 0.5, 'assets': 0.8, 'liabilities': 0.6},\n",
        "    }\n",
        "\n",
        "    # Generate headquarters data\n",
        "    hq_revenue = np.random.normal(loc=1000000, scale=200000, size=num_samples) * np.vectorize(lambda x: industry_multipliers[x]['revenue'])(hq_industry)\n",
        "    hq_cost = hq_revenue * np.random.uniform(0.5, 0.7, num_samples) * np.vectorize(lambda x: industry_multipliers[x]['cost'])(hq_industry)\n",
        "    hq_profit = hq_revenue - hq_cost\n",
        "    hq_assets = np.random.normal(loc=500000, scale=100000, size=num_samples) * np.vectorize(lambda x: industry_multipliers[x]['assets'])(hq_industry)\n",
        "    hq_liabilities = hq_assets * np.random.uniform(0.3, 0.5, num_samples) * np.vectorize(lambda x: industry_multipliers[x]['liabilities'])(hq_industry)\n",
        "\n",
        "    # Generate subsidiary data\n",
        "    subs_revenue = hq_revenue * np.random.uniform(0.2, 0.4, num_samples) * np.vectorize(lambda x: industry_multipliers[x]['revenue'])(subs_industry)\n",
        "    subs_cost = subs_revenue * np.random.uniform(0.6, 0.8, num_samples) * np.vectorize(lambda x: industry_multipliers[x]['cost'])(subs_industry)\n",
        "    subs_profit = subs_revenue - subs_cost\n",
        "    subs_assets = hq_assets * np.random.uniform(0.1, 0.3, num_samples) * np.vectorize(lambda x: industry_multipliers[x]['assets'])(subs_industry)\n",
        "    subs_liabilities = subs_assets * np.random.uniform(0.4, 0.6, num_samples) * np.vectorize(lambda x: industry_multipliers[x]['liabilities'])(subs_industry)\n",
        "\n",
        "    # Generate intercompany transaction data\n",
        "    intercompany_sales = subs_revenue * np.random.uniform(0.1, 0.3, num_samples)\n",
        "    intercompany_purchases = intercompany_sales * np.random.uniform(0.8, 1.2, num_samples)\n",
        "\n",
        "    # Define additional factors based on OECD guidelines\n",
        "    hq_functions = np.random.uniform(0.1, 0.5, num_samples)\n",
        "    subs_functions = np.random.uniform(0.2, 0.6, num_samples)\n",
        "    hq_risks = np.random.uniform(0.1, 0.3, num_samples)\n",
        "    subs_risks = np.random.uniform(0.2, 0.4, num_samples)\n",
        "\n",
        "    # Calculate profit allocation key using a combination of factors\n",
        "    total_assets = hq_assets + subs_assets\n",
        "    total_liabilities = hq_liabilities + subs_liabilities\n",
        "    total_functions = hq_functions + subs_functions\n",
        "    total_risks = hq_risks + subs_risks\n",
        "\n",
        "    hq_weighted_factors = (hq_assets / total_assets) * 0.25 + (hq_liabilities / total_liabilities) * 0.15 + (hq_functions / total_functions) * 0.3 + (hq_risks / total_risks) * 0.3\n",
        "    subs_weighted_factors = (subs_assets / total_assets) * 0.25 + (subs_liabilities / total_liabilities) * 0.15 + (subs_functions / total_functions) * 0.3 + (subs_risks / total_risks) * 0.3\n",
        "\n",
        "    profit_allocation_key = subs_weighted_factors / (hq_weighted_factors + subs_weighted_factors)\n",
        "\n",
        "    # Compile the data into a DataFrame\n",
        "    data = {\n",
        "        'hq_revenue': hq_revenue,\n",
        "        'hq_cost': hq_cost,\n",
        "        'hq_profit': hq_profit,\n",
        "        'hq_assets': hq_assets,\n",
        "        'hq_liabilities': hq_liabilities,\n",
        "        'subs_revenue': subs_revenue,\n",
        "        'subs_cost': subs_cost,\n",
        "        'subs_profit': subs_profit,\n",
        "        'subs_assets': subs_assets,\n",
        "        'subs_liabilities': subs_liabilities,\n",
        "        'hq_functions': hq_functions,\n",
        "        'subs_functions': subs_functions,\n",
        "        'hq_risks': hq_risks,\n",
        "        'subs_risks': subs_risks,\n",
        "        'hq_industry': hq_industry,\n",
        "        'subs_industry': subs_industry,\n",
        "        'hq_function': hq_function,\n",
        "        'subs_function': subs_function,\n",
        "        'profit_allocation_key': profit_allocation_key,\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Generate the dataset"
      ],
      "metadata": {
        "id": "SmYTXsiHquzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare the Data\n",
        "# One-hot encode the categorical variables\n",
        "df_encoded = pd.get_dummies(df, columns=['hq_industry', 'subs_industry', 'hq_function', 'subs_function'])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df_encoded.drop(columns=['profit_allocation_key'])\n",
        "y = df_encoded['profit_allocation_key']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "pN9FJHYLqw4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build the Neural Network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# Compile the model with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['mean_absolute_error'])\n"
      ],
      "metadata": {
        "id": "z7iMs-q7q1jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the Model with early stopping\n",
        "history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_data=(X_val_scaled, y_val))"
      ],
      "metadata": {
        "id": "V_bw2ZL9q4cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Evaluate the Model\n",
        "test_loss, test_mae = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')"
      ],
      "metadata": {
        "id": "YaE5udSIq6ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Predict on Test Data and Plot\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted Profit Allocation Key')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N71seiaZq_hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Create a DataFrame for Predictions vs Actual\n",
        "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})\n",
        "print(results_df.head(10))"
      ],
      "metadata": {
        "id": "IF0C6FMSrFVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "\n",
        "output_dir = \"model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "model.save('model/profit_split_model.keras')"
      ],
      "metadata": {
        "id": "u3qTcJ9xq9Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "BUKd61s1s732"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=keras /content/model/profit_split_model.h5 content/model/profit_split_tfjs_model"
      ],
      "metadata": {
        "id": "Y_iPh0tgp-zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder les param√®tres du scaler\n",
        "scaler_params = {\n",
        "    'mean': scaler.mean_.tolist(),\n",
        "    'std': scaler.scale_.tolist()\n",
        "}\n",
        "\n",
        "with open('scaler_params.json', 'w') as f:\n",
        "    json.dump(scaler_params, f)"
      ],
      "metadata": {
        "id": "W_G8h256GpOa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}